{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scopus Authors\n",
    "### This is an academic project for authors data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import mysql.connector\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import userip as ui # this is the module I created that includes classes for getting the list of different user agents and ips\n",
    "from itertools import cycle\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MySql connector\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"shahab\",\n",
    "    database=\"datascopus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the database connection\n",
    "if mydb:\n",
    "    print(\"connected\")\n",
    "else:\n",
    "    print(\"NOT connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ui.UserAgent()\n",
    "b = ui.Ip()\n",
    "users = a.get_list()\n",
    "ips = b.get_ip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ips: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"number of ips: {}\".format(len(ips))) #sometimes there is no ips that matches our conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ips[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of user agents: 10839\n"
     ]
    }
   ],
   "source": [
    "print(\"number of user agents: {}\".format(len(users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.1; AOLBuild 4334.5000; Windows NT 5.1; Trident/4.0)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[100] #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = mydb.cursor()\n",
    "df = pd.read_csv(\"authors.csv\") #authors CSV file\n",
    "df = df.dropna(axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_array = df.loc[:, \"author_id\"].values\n",
    "authors_list = authors_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_pool = cycle(ips)\n",
    "user_pool = cycle(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can run loop over some ips and user agents. (some of the user agents do not work)\n",
    "### It is better to test the ones that work in advance and then create a list and run a loop over that.\n",
    "### I created a module that extracts 10839 user agents and some ips (if available) from these two websites:\n",
    "### 1)  https://free-proxy-list.net\n",
    "### 2)  www.useragentstring.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for auth in authors_list:\n",
    "    url = \"https://www.scopus.com/authid/detail.uri?authorId={}\".format(auth)\n",
    "    print(auth)\n",
    "    a = \"wrong\"\n",
    "    while a != \"right\":\n",
    "        try:\n",
    "            response = requests.get(url,headers={'User-Agent': \"Mozilla/5.0 (compatible; U; ABrowse 0.6;  Syllable) AppleWebKit/420+ (KHTML, like Gecko)\"})\n",
    "            #response = requests.get(url,headers={'User-Agent': next(user_pool)})\n",
    "            # response = requests.get(url,headers={'User-Agent': next(user_pool)}, proxies = {\"https\" : next(proxy_pool)})\n",
    "            a = \"right\" \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    page_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    first = page_soup.findAll(\"h2\", {\"class\" : \"wordBreakWord\"})\n",
    "    full_name = first[0].text.replace(\",\\xa0\", \" \").replace(\"\\r\", \" \").replace(\"Is this you? Claim profile  Opens in new window\", \" \").strip()\n",
    "    print(full_name)\n",
    "    \n",
    "    other_names = page_soup.findAll(\"div\", {\"id\" : \"otherNameFormatBadges\"})\n",
    "    span_list = other_names[0].findAll(\"span\")\n",
    "    other_names = []\n",
    "    for i in span_list:\n",
    "        other_names.append(i.text.strip())\n",
    "    others = \"|\".join(other_names).replace(\"|\", \"| \") \n",
    "    t = len(other_names)\n",
    "    print(others)\n",
    "        \n",
    "\n",
    "    university = page_soup.findAll(\"div\", {\"class\" : \"authAffilcityCounty\"})[0].text.strip().replace(\"\\r\", \" \")\n",
    "    print(university)\n",
    "\n",
    "    subject_areas = page_soup.findAll(\"span\", {\"class\" : \"badges\"})\n",
    "    subject_areas = subject_areas[t:]\n",
    "    \n",
    "\n",
    "    areas = []\n",
    "    for i in range(len(subject_areas)):\n",
    "        areas.append(subject_areas[i].text.strip().replace(\",\", \"\"))\n",
    "\n",
    "    subjects = \",\".join(areas).replace(\",\", \", \")\n",
    "    print(subjects)\n",
    "\n",
    "    documents_by_author = page_soup.findAll(\"span\", {\"class\" : \"fontLarge pull-left\"})[0].text\n",
    "    print(documents_by_author)\n",
    "\n",
    "    total_citation_number = page_soup.findAll(\"span\", {\"class\" : \"fontLarge darkGrayText\"})[0].text\n",
    "    print(total_citation_number)\n",
    "\n",
    "    total_document = page_soup.findAll(\"div\", {\"class\" : \"lightGreyText\"})[0].findAll(\"span\")[1].text\n",
    "    print(total_document)\n",
    "\n",
    "    h_index = page_soup.findAll(\"span\", {\"class\" : \"fontLarge\"})[0].text\n",
    "    print(h_index)\n",
    "    \n",
    "    sql = \"\"\"INSERT INTO authors(full_name, others, university, subjects, document_by_author, total_citation_numebr, total_document, h_index) VALUES (\n",
    "    \"{}\", \"{}\", \"{}\", \"{}\", \"{}\", \"{}\", \"{}\", \"{}\")\"\"\".format(full_name, others, university, subjects, int(documents_by_author),\n",
    "                                                       int(total_citation_number), int(total_document), int(h_index))\n",
    "    \n",
    "    try:\n",
    "        mycursor.execute(sql)\n",
    "        mydb.commit()\n",
    "    except:\n",
    "        mydb.rollback()\n",
    "        print(\"NOT connected\")\n",
    "    time.sleep(20) # 20 seconds sleep\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watch",
   "language": "python",
   "name": "watch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
